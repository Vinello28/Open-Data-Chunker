# Open Data Chunker

[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)
[![Docker](https://img.shields.io/badge/docker-ready-brightgreen.svg)](https://www.docker.com/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A high-performance ETL pipeline for processing large-scale XML datasets from the Italian RNA (Registro Nazionale degli Aiuti) Open Data. Transforms XML files into optimized, year-partitioned Parquet format with built-in query and export capabilities.

## âœ¨ Features

- **ğŸš€ High-Performance Parsing** â€” Multi-worker parallel processing with streaming XML parser
- **ğŸ›¡ï¸ Fault-Tolerant** â€” Automatic recovery from malformed XML with invalid character sanitization
- **ğŸ“¦ Parquet Output** â€” Columnar storage with Hive-style partitioning (`ANNO=YYYY`)
- **ğŸ” SQL Queries** â€” Interactive DuckDB-powered queries on processed datasets
- **ğŸ“¤ Flexible Export** â€” CSV/TXT export with configurable delimiters
- **ğŸ³ Dockerized** â€” Fully containerized for reproducible environments

## ğŸ—ï¸ Architecture

```mermaid
flowchart LR
    subgraph Input
        XML[("ğŸ“„ XML Files<br/>data/*.xml")]
    end

    subgraph Processing
        Parser["ğŸ”§ Parser<br/>(lxml + Sanitizer)"]
    end

    subgraph Storage
        Parquet[("ğŸ“¦ Parquet<br/>public/parquet/")]
    end

    subgraph CLI["âš¡ CLI (Click)"]
        Query["ğŸ” query"]
        Export["ğŸ“¤ export"]
    end

    subgraph Engines
        DuckDB["ğŸ¦† DuckDB"]
        Polars["ğŸ»â€â„ï¸ Polars"]
    end

    XML --> Parser --> Parquet
    Parquet --> Query --> DuckDB
    Parquet --> Export --> Polars
```

### Data Model

The pipeline extracts three normalized tables from the XML:

| Table | Description |
|-------|-------------|
| `aiuti` | Main grants/subsidies records |
| `componenti` | Aid component details (linked to `aiuti`) |
| `strumenti` | Financial instruments (linked to `componenti`) |

## ğŸš€ Quick Start

### Prerequisites

- Docker Engine â‰¥ 20.10
- Docker Compose â‰¥ 2.0

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/Open-Data-Chunker.git
cd Open-Data-Chunker

# Build the Docker image
docker compose build
```

### Basic Usage

```bash
# Parse a single XML file
docker compose run --rm etl python -m src.cli parse --input data/2022/OpenData_Aiuti_2022_08.xml

# Parse entire data directory with 8 parallel workers
docker compose run --rm etl python -m src.cli parse --input data/ --workers 8
```

Output is written to `public/parquet/{table}/ANNO=YYYY/`.

## ğŸ“– CLI Reference

### `parse` â€” Process XML Files

```bash
docker compose run --rm etl python -m src.cli parse [OPTIONS]
```

| Option | Description | Default |
|--------|-------------|---------|
| `-i, --input` | Input file or directory (required) | â€” |
| `-o, --output` | Output directory for Parquet | `public/parquet` |
| `-w, --workers` | Number of parallel workers | `4` |

**Example:**
```bash
docker compose run --rm etl python -m src.cli parse \
  --input data/ \
  --output public/parquet \
  --workers 8
```

---

### `query` â€” Run SQL Queries

```bash
docker compose run --rm etl python -m src.cli query [OPTIONS]
```

| Option | Description | Default |
|--------|-------------|---------|
| `-t, --table` | Table to query (`aiuti`, `componenti`, `strumenti`) | required |
| `-q, --query` | Custom SQL query (DuckDB syntax) | â€” |
| `-l, --limit` | Limit results | `10` |

**Examples:**
```bash
# View first 5 records from aiuti
docker compose run --rm etl python -m src.cli query --table aiuti --limit 5

# Custom aggregation query
docker compose run --rm etl python -m src.cli query \
  --table strumenti \
  --query "SELECT ANNO, SUM(ELEMENTO_DI_AIUTO) as total FROM read_parquet('public/parquet/strumenti/**/*.parquet') GROUP BY ANNO ORDER BY ANNO"
```

---

### `export` â€” Export to CSV/TXT

```bash
docker compose run --rm etl python -m src.cli export [OPTIONS]
```

| Option | Description | Default |
|--------|-------------|---------|
| `-t, --table` | Table to export | required |
| `-f, --format` | Output format (`csv`, `txt`) | `csv` |
| `-o, --output` | Output file path | required |
| `-d, --delimiter` | Field delimiter | `,` |

**Examples:**
```bash
# Export to CSV
docker compose run --rm etl python -m src.cli export \
  --table aiuti \
  --format csv \
  --output public/exports/aiuti.csv

# Export with pipe delimiter
docker compose run --rm etl python -m src.cli export \
  --table strumenti \
  --format txt \
  --delimiter "|" \
  --output public/exports/strumenti.txt
```

## ğŸ“ Project Structure

```
Open-Data-Chunker/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ cli.py          # Click CLI entry point
â”‚   â”œâ”€â”€ parser.py       # XML parsing with CleanFileInputStream
â”‚   â”œâ”€â”€ exporter.py     # Query execution & export logic
â”‚   â””â”€â”€ models.py       # PyArrow schema definitions
â”œâ”€â”€ data/               # Input XML files (gitignored)
â”œâ”€â”€ public/
â”‚   â”œâ”€â”€ parquet/        # Output Parquet datasets
â”‚   â””â”€â”€ exports/        # Exported CSV/TXT files
â”œâ”€â”€ tests/
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ usage.md
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ pyproject.toml
â””â”€â”€ requirements.txt
```

## ğŸ› ï¸ Development

### Local Setup (without Docker)

```bash
# Create virtual environment
python -m venv .venv
source .venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Run CLI directly
python -m src.cli parse --input data/ --workers 4
```

### Running Tests

```bash
docker compose run --rm etl pytest tests/
```

## âš™ï¸ Configuration

### Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `PYTHONUNBUFFERED` | Force unbuffered stdout/stderr | `1` (set in Docker) |

### Parser Tuning

Adjustable constants in `src/parser.py`:

```python
BATCH_SIZE = 10000  # Records per Parquet file write
```

## ğŸ”§ Technical Details

### XML Sanitization

The `CleanFileInputStream` wrapper automatically removes invalid XML characters:
- Control characters `0x00-0x08`, `0x0B`, `0x0C`, `0x0E-0x1F`
- Malformed numeric entities (`&#0;` through `&#31;`, except `&#9;`, `&#10;`, `&#13;`)

### Memory Management

Uses `lxml.etree.iterparse()` with aggressive element cleanup:
```python
elem.clear()
while elem.getprevious() is not None:
    del elem.getparent()[0]
```

### Parallel Processing

Worker processes use `ProcessPoolExecutor` with configurable `--workers` option:
- CPU-bound: Scales linearly with available cores
- I/O-bound: Benefits from moderate parallelism

## ğŸ“Š Performance

Tested on a dataset of ~165 XML files:

| Metric | Value |
|--------|-------|
| Parse Speed | ~50k records/sec (8 workers) |
| Memory Usage | ~200MB per worker |
| Output Compression | ~10x vs raw XML |

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

Distributed under the MIT License. See `LICENSE` for more information.

## ğŸ™ Acknowledgments

- [RNA Open Data](https://www.rna.gov.it/) â€” Italian State Aid Registry
- [lxml](https://lxml.de/) â€” High-performance XML processing
- [Polars](https://www.pola.rs/) â€” Lightning-fast DataFrames
- [DuckDB](https://duckdb.org/) â€” In-process analytical database
